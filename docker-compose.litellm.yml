version: '3'

services:
  # This extends the main docker-compose.yml with litellm services
  
  # LiteLLM as an MCP-compatible API proxy
  litellm:
    # Use Python image directly instead of the ARM64-only image
    image: python:3.10-slim
    restart: unless-stopped
    ports:
      - "8000:8000"  # Main API port
    command: >
      sh -c "
      pip install --no-cache-dir litellm fastapi uvicorn &&
      python -m litellm --model ollama/codexcontinue --api_base http://ollama:11434 --port 8000 --host 0.0.0.0
      "
    environment:
      - OLLAMA_API_BASE=http://ollama:11434
      - MODEL_NAME=codexcontinue
      - OPENAI_API_KEY=sk-no-key-required  # Dummy key
      - PORT=8000
      - HOST=0.0.0.0
    depends_on:
      - ollama
    networks:
      - codexcontinue-network
      
  # RAG proxy service - This will be our custom service that adds RAG capabilities to the MCP server
  rag-proxy:
    # Use a simple Python image directly
    image: python:3.10-slim
    ports:
      - "5001:5001"  # RAG proxy port
    environment:
      - PYTHONPATH=/app
      - VECTOR_DB_PATH=/app/data/vectorstore
      - KNOWLEDGE_BASE_PATH=/app/data/knowledge_base
      - LITELLM_API_URL=http://litellm:8000
      - RAG_PROXY_PORT=5001
    depends_on:
      - litellm
    volumes:
      - ./ml:/app/ml
      - vector-data:/app/data/vectorstore
      - knowledge-data:/app/data/knowledge_base
    networks:
      - codexcontinue-network
    # Install Flask and run our simplified direct app
    command: >
      sh -c "
      pip install --no-cache-dir flask flask-cors requests &&
      python /app/ml/app_mcp_rag_direct.py
      "
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96940127",
   "metadata": {},
   "source": [
    "# WSL GPU Setup Guide for CodexContinue\n",
    "\n",
    "This notebook provides a step-by-step guide to set up GPU support in Windows Subsystem for Linux (WSL) for the CodexContinue project. By following these instructions, you'll be able to use your NVIDIA GeForce RTX 2070 GPU with Ollama in WSL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b034a4",
   "metadata": {},
   "source": [
    "## Current Status\n",
    "\n",
    "Based on the diagnostics, we've identified the following:\n",
    "\n",
    "1. ✅ You have an NVIDIA GeForce RTX 2070 with 8GB VRAM on your Windows system\n",
    "2. ✅ You've installed the CUDA repository in WSL\n",
    "3. ✅ You've installed the NVIDIA Container Toolkit in WSL\n",
    "4. ❌ The NVIDIA driver for WSL is not installed or not working properly\n",
    "\n",
    "The key issue is that WSL cannot find your GPU: `WSL environment detected but no adapters were found`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b824c7",
   "metadata": {},
   "source": [
    "## Step 1: Install NVIDIA Driver for WSL on Windows\n",
    "\n",
    "The most important step is to install the special NVIDIA driver for WSL on your Windows host system.\n",
    "\n",
    "1. Open a web browser on Windows (not in WSL)\n",
    "2. Go to: https://developer.nvidia.com/cuda/wsl\n",
    "3. Click \"Download Now\" to download the NVIDIA GPU driver for WSL\n",
    "4. Run the installer on your Windows system\n",
    "5. Restart your computer after installation\n",
    "\n",
    "This driver is different from the regular NVIDIA drivers. It's specifically designed to enable GPU access from WSL.\n",
    "\n",
    "**Note:** The regular NVIDIA driver you have installed (version 457.51) may be too old to support WSL GPU features properly. The WSL-specific driver installation should update this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29c5da",
   "metadata": {},
   "source": [
    "## Step 2: Verify GPU Support in WSL\n",
    "\n",
    "After installing the driver and restarting your computer, open your WSL terminal and check if the GPU is detected:\n",
    "\n",
    "```bash\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "If properly installed, you should see information about your RTX 2070 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can be run to check if the NVIDIA driver is properly installed in WSL\n",
    "# You'll need to run it from the Jupyter notebook running in WSL\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_nvidia_driver():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ NVIDIA driver is properly installed and working!\")\n",
    "            print(\"\\nNVIDIA Driver Output:\")\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ NVIDIA driver is not working properly.\")\n",
    "            print(\"\\nError:\")\n",
    "            print(result.stderr)\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ NVIDIA driver (nvidia-smi) is not installed or not in PATH.\")\n",
    "        return False\n",
    "\n",
    "check_nvidia_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916e497",
   "metadata": {},
   "source": [
    "## Step 3: Test GPU with Docker\n",
    "\n",
    "After confirming that `nvidia-smi` works, test if Docker can access the GPU:\n",
    "\n",
    "```bash\n",
    "docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi\n",
    "```\n",
    "\n",
    "If successful, you'll see the same GPU information output from within the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7720b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can be run to check if Docker can access the GPU\n",
    "# You'll need to run it from the Jupyter notebook running in WSL\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def check_docker_gpu():\n",
    "    try:\n",
    "        print(\"Testing GPU access from Docker container...\")\n",
    "        result = subprocess.run(\n",
    "            ['docker', 'run', '--rm', '--gpus', 'all', 'nvidia/cuda:11.6.2-base-ubuntu20.04', 'nvidia-smi'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Docker can access the GPU!\")\n",
    "            print(\"\\nOutput from container:\")\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Docker cannot access the GPU.\")\n",
    "            print(\"\\nError:\")\n",
    "            print(result.stderr)\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error running Docker command: {e}\")\n",
    "        return False\n",
    "\n",
    "check_docker_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95843c30",
   "metadata": {},
   "source": [
    "## Step 4: Start Ollama with GPU Support\n",
    "\n",
    "Once Docker can access the GPU, you can start Ollama with GPU acceleration using the provided script:\n",
    "\n",
    "```bash\n",
    "./scripts/start-ollama-wsl.sh\n",
    "```\n",
    "\n",
    "This script is configured to use the GPU for Ollama in WSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58939f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can be run to start Ollama with GPU support\n",
    "# You'll need to run it from the Jupyter notebook running in WSL\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def start_ollama_with_gpu():\n",
    "    script_path = os.path.join(os.getcwd(), 'scripts', 'start-ollama-wsl.sh')\n",
    "    \n",
    "    if not os.path.exists(script_path):\n",
    "        print(f\"❌ Script not found: {script_path}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting Ollama with GPU support...\")\n",
    "        # Using Popen to allow the process to run in the background\n",
    "        process = subprocess.Popen(\n",
    "            ['bash', script_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Print the first few lines of output\n",
    "        print(\"\\nInitial output:\")\n",
    "        for i, line in enumerate(process.stdout):\n",
    "            print(line.strip())\n",
    "            if i >= 10:  # Only show first 10 lines\n",
    "                print(\"...(output continues)...\")\n",
    "                break\n",
    "                \n",
    "        print(\"\\n✅ Ollama startup process initiated!\")\n",
    "        print(\"Check Docker containers to verify it's running: docker ps\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error starting Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment the next line to run this function\n",
    "# start_ollama_with_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d225c0",
   "metadata": {},
   "source": [
    "## Step 5: Run the Full CodexContinue Environment\n",
    "\n",
    "After confirming Ollama is running with GPU support, you can start the complete development environment:\n",
    "\n",
    "```bash\n",
    "./scripts/start-dev-environment.sh\n",
    "```\n",
    "\n",
    "This will start all the necessary services for the CodexContinue project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708b1e7",
   "metadata": {},
   "source": [
    "## Alternative: CPU-Only Mode\n",
    "\n",
    "If you're having persistent issues with GPU setup, you can run CodexContinue in CPU-only mode:\n",
    "\n",
    "```bash\n",
    "docker compose -f docker-compose.yml -f docker-compose.macos.yml up\n",
    "```\n",
    "\n",
    "This uses the macOS configuration, which is designed for CPU-only operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a29b8c",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **WSL NVIDIA Driver Not Detected**: \n",
    "   - Make sure you've installed the NVIDIA driver for WSL on Windows (not in WSL)\n",
    "   - Verify your GPU is compatible with WSL GPU acceleration\n",
    "\n",
    "2. **Docker GPU Access Issues**:\n",
    "   - Verify the NVIDIA Container Toolkit is installed: `nvidia-ctk --version`\n",
    "   - Check Docker configuration: `cat /etc/docker/daemon.json`\n",
    "   - Make sure Docker has been restarted after configuration\n",
    "\n",
    "3. **No GPU Found in Container**:\n",
    "   - Check that `nvidia-smi` works in WSL\n",
    "   - Verify Docker is configured with GPU access\n",
    "\n",
    "### Run the Troubleshooting Script\n",
    "\n",
    "The CodexContinue project provides a troubleshooting script:\n",
    "\n",
    "```bash\n",
    "./scripts/troubleshoot-wsl-gpu.sh\n",
    "```\n",
    "\n",
    "This script checks your WSL GPU setup and provides guidance for fixing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can run the WSL GPU troubleshooting script\n",
    "# You'll need to run it from the Jupyter notebook running in WSL\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_troubleshooting_script():\n",
    "    script_path = os.path.join(os.getcwd(), 'scripts', 'troubleshoot-wsl-gpu.sh')\n",
    "    \n",
    "    if not os.path.exists(script_path):\n",
    "        print(f\"❌ Script not found: {script_path}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(\"Running WSL GPU troubleshooting script...\")\n",
    "        result = subprocess.run(\n",
    "            ['bash', script_path],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\nTroubleshooting output:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"\\n✅ Troubleshooting completed!\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️ Troubleshooting script exited with code {result.returncode}\")\n",
    "            if result.stderr:\n",
    "                print(\"\\nErrors:\")\n",
    "                print(result.stderr)\n",
    "        \n",
    "        return result.returncode == 0\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error running troubleshooting script: {e}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment the next line to run the troubleshooting script\n",
    "# run_troubleshooting_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0daa96",
   "metadata": {},
   "source": [
    "## Using VS Code DevContainer\n",
    "\n",
    "After setting up GPU support, you can use VS Code's DevContainer feature for development:\n",
    "\n",
    "1. Open the CodexContinue project in VS Code\n",
    "2. Click the green icon in the bottom-left corner (or press F1 and search for \"Remote-Containers\")\n",
    "3. Select \"Reopen in Container\"\n",
    "\n",
    "The DevContainer configuration has been optimized for WSL with the volume mount fixes already implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17c8fc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By following these steps, you should be able to use your NVIDIA GeForce RTX 2070 GPU with the CodexContinue project in WSL. The GPU acceleration will significantly improve the performance of the Ollama language model.\n",
    "\n",
    "Remember that the most critical step is installing the NVIDIA driver for WSL on your Windows host system, not in WSL itself."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a99893",
   "metadata": {},
   "source": [
    "# NVIDIA GPU Setup Guide for WSL in CodexContinue\n",
    "\n",
    "This notebook provides a step-by-step guide for diagnosing and fixing NVIDIA GPU support in Windows Subsystem for Linux (WSL) for the CodexContinue project. The guide focuses on resolving common issues with missing libraries and ensuring proper Docker integration with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bd8c7",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import the necessary Python libraries for system checks and diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77348767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure IPython is installed\n",
    "%pip install ipython\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For prettier output\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a50f7",
   "metadata": {},
   "source": [
    "## Check NVIDIA Libraries in WSL\n",
    "\n",
    "Let's check for the presence of NVIDIA libraries in the standard WSL location (`/usr/lib/wsl/lib/libnvidia-ml.so`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nvidia_libraries():\n",
    "    wsl_lib_path = \"/usr/lib/wsl/lib/libnvidia-ml.so\"\n",
    "    if os.path.exists(wsl_lib_path):\n",
    "        display(Markdown(f\"✅ Found NVIDIA libraries in WSL driver location: `{wsl_lib_path}`\"))\n",
    "    else:\n",
    "        display(Markdown(f\"❌ Missing NVIDIA libraries in standard WSL location: `{wsl_lib_path}`\"))\n",
    "        \n",
    "    # Check LD_LIBRARY_PATH\n",
    "    ld_library_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "    display(Markdown(f\"Current LD_LIBRARY_PATH: `{ld_library_path}`\"))\n",
    "\n",
    "check_nvidia_libraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bdc71",
   "metadata": {},
   "source": [
    "## Verify `nvidia-smi` Output\n",
    "\n",
    "Now, let's run the `nvidia-smi` command to verify GPU status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nvidia_smi():\n",
    "    try:\n",
    "        result = subprocess.run(['which', 'nvidia-smi'], capture_output=True, text=True, check=False)\n",
    "        if result.returncode == 0:\n",
    "            display(Markdown(f\"Found nvidia-smi at: `{result.stdout.strip()}`\"))\n",
    "            \n",
    "            # Run nvidia-smi\n",
    "            nvidia_smi_output = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=False)\n",
    "            if nvidia_smi_output.returncode == 0:\n",
    "                display(Markdown(\"### nvidia-smi output:\"))\n",
    "                print(nvidia_smi_output.stdout)\n",
    "                return True\n",
    "            else:\n",
    "                display(Markdown(f\"❌ Error running nvidia-smi: {nvidia_smi_output.stderr}\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ nvidia-smi command not found\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ Error: {str(e)}\"))\n",
    "    return False\n",
    "\n",
    "ran_successfully = run_nvidia_smi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9394cfd",
   "metadata": {},
   "source": [
    "## Search for NVIDIA Libraries\n",
    "\n",
    "Let's search for `libnvidia-ml.so` in common library paths and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nvidia_libraries():\n",
    "    display(Markdown(\"Looking for libnvidia-ml.so in common paths:\"))\n",
    "    \n",
    "    search_paths = [\n",
    "        \"/usr/lib\",\n",
    "        \"/usr/lib/x86_64-linux-gnu\",\n",
    "        \"/usr/lib/wsl/lib\",\n",
    "        \"/usr/local/cuda*/targets/x86_64-linux/lib\"\n",
    "    ]\n",
    "    \n",
    "    found_libs = []\n",
    "    \n",
    "    for path in search_paths:\n",
    "        # Handle glob patterns\n",
    "        if '*' in path:\n",
    "            matching_dirs = glob.glob(path)\n",
    "            for dir in matching_dirs:\n",
    "                lib_file = os.path.join(dir, \"libnvidia-ml.so\")\n",
    "                if os.path.exists(lib_file):\n",
    "                    found_libs.append(lib_file)\n",
    "                # Check for stub files\n",
    "                stub_path = os.path.join(dir, \"stubs/libnvidia-ml.so\")\n",
    "                if os.path.exists(stub_path):\n",
    "                    found_libs.append(stub_path + \" (stub)\")\n",
    "        else:\n",
    "            lib_file = os.path.join(path, \"libnvidia-ml.so\")\n",
    "            if os.path.exists(lib_file):\n",
    "                found_libs.append(lib_file)\n",
    "    \n",
    "    if found_libs:\n",
    "        for lib in found_libs:\n",
    "            display(Markdown(f\"- Found `{lib}`\"))\n",
    "    else:\n",
    "        display(Markdown(\"❌ No NVIDIA libraries found in common paths\"))\n",
    "\n",
    "search_nvidia_libraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008d2e5",
   "metadata": {},
   "source": [
    "## Check NVIDIA Container Toolkit Configuration\n",
    "\n",
    "Let's read and parse the Docker daemon configuration file to verify NVIDIA runtime settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_docker_nvidia_config():\n",
    "    docker_config_path = \"/etc/docker/daemon.json\"\n",
    "    \n",
    "    if os.path.exists(docker_config_path):\n",
    "        try:\n",
    "            with open(docker_config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "            display(Markdown(\"### Docker daemon configuration:\"))\n",
    "            print(json.dumps(config, indent=4))\n",
    "            \n",
    "            if 'runtimes' in config and 'nvidia' in config['runtimes']:\n",
    "                display(Markdown(\"✅ NVIDIA Container Runtime is configured\"))\n",
    "                return True\n",
    "            else:\n",
    "                display(Markdown(\"❌ NVIDIA Container Runtime is not configured\"))\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"Error reading Docker configuration: {str(e)}\"))\n",
    "    else:\n",
    "        display(Markdown(f\"❌ Docker daemon configuration file not found at {docker_config_path}\"))\n",
    "    \n",
    "    return False\n",
    "\n",
    "docker_configured = check_docker_nvidia_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a0f45",
   "metadata": {},
   "source": [
    "## Test Docker GPU Capability\n",
    "\n",
    "Now let's run a Docker command to check for GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ddb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_docker_gpu():\n",
    "    display(Markdown(\"### Testing Docker with GPU support:\"))\n",
    "    \n",
    "    # First check docker info for nvidia runtime\n",
    "    try:\n",
    "        docker_info = subprocess.run(['docker', 'info'], capture_output=True, text=True, check=False)\n",
    "        if 'nvidia' in docker_info.stdout and 'Runtimes' in docker_info.stdout:\n",
    "            display(Markdown(\"✅ Docker info shows nvidia runtime is available\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ Docker info does not show nvidia runtime\"))\n",
    "            print(\"Docker runtimes found:\")\n",
    "            for line in docker_info.stdout.splitlines():\n",
    "                if 'Runtimes' in line:\n",
    "                    print(line)\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error checking Docker info: {str(e)}\"))\n",
    "    \n",
    "    # Try running a container with GPU access\n",
    "    try:\n",
    "        display(Markdown(\"### Testing GPU access with a CUDA container:\"))\n",
    "        display(Markdown(\"Running: `docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi`\"))\n",
    "        \n",
    "        # Run the command but don't capture output as it might be large\n",
    "        display(Markdown(\"This might take a moment to pull the container image if it's not already cached...\"))\n",
    "        result = subprocess.run(\n",
    "            ['docker', 'run', '--rm', '--gpus', 'all', 'nvidia/cuda:11.6.2-base-ubuntu20.04', 'nvidia-smi'],\n",
    "            capture_output=True, text=True, check=False\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            display(Markdown(\"✅ Successfully ran nvidia-smi in a Docker container with GPU access\"))\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            display(Markdown(f\"❌ Failed to run nvidia-smi in Docker: {result.stderr}\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error testing Docker GPU support: {str(e)}\"))\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Only run if the user wants to test Docker\n",
    "test_docker = input(\"Do you want to test Docker with GPU support? (y/n): \")\n",
    "if test_docker.lower() == 'y':\n",
    "    docker_test_successful = test_docker_gpu()\n",
    "else:\n",
    "    display(Markdown(\"Skipping Docker GPU test\"))\n",
    "    docker_test_successful = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9120e8",
   "metadata": {},
   "source": [
    "## Provide Fix Suggestions\n",
    "\n",
    "Based on the diagnostic results, here are actionable suggestions to fix common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_fixes():\n",
    "    display(Markdown(\"## Diagnostic Summary and Fix Suggestions\"))\n",
    "    \n",
    "    # Check if NVIDIA libraries exist in /usr/lib/wsl/lib\n",
    "    wsl_lib_path = \"/usr/lib/wsl/lib/libnvidia-ml.so\"\n",
    "    wsl_lib_exists = os.path.exists(wsl_lib_path)\n",
    "    \n",
    "    # Check if we found nvidia-smi earlier\n",
    "    nvidia_smi_runs = ran_successfully\n",
    "    \n",
    "    # Check Docker configuration\n",
    "    docker_configured_correctly = docker_configured\n",
    "    \n",
    "    # Print summary\n",
    "    display(Markdown(\"### Status Summary:\"))\n",
    "    display(Markdown(f\"- NVIDIA libraries in WSL path: {'✅ Found' if wsl_lib_exists else '❌ Missing'}\"))\n",
    "    display(Markdown(f\"- nvidia-smi runs successfully: {'✅ Yes' if nvidia_smi_runs else '❌ No'}\"))\n",
    "    display(Markdown(f\"- Docker NVIDIA runtime configured: {'✅ Yes' if docker_configured_correctly else '❌ No'}\"))\n",
    "    \n",
    "    # Generate fix suggestions\n",
    "    display(Markdown(\"### Fix Suggestions:\"))\n",
    "    \n",
    "    fixes = []\n",
    "    \n",
    "    if not wsl_lib_exists and nvidia_smi_runs:\n",
    "        # We have NVIDIA drivers but missing the symbolic link\n",
    "        fixes.append(\"**Missing Library Symbolic Link**:\"\n",
    "                  \"\\n1. Run the fix script: `sudo ./scripts/fix-nvidia-wsl-libs.sh`\"\n",
    "                  \"\\n2. This will create the symbolic link from an existing library to the standard WSL location\")\n",
    "    \n",
    "    if not nvidia_smi_runs:\n",
    "        fixes.append(\"**NVIDIA Driver Issues**:\"\n",
    "                  \"\\n1. Verify you have the NVIDIA driver for WSL installed on Windows (not in WSL)\"\n",
    "                  \"\\n2. Install from: https://developer.nvidia.com/cuda/wsl\"\n",
    "                  \"\\n3. Restart your Windows system after installation\"\n",
    "                  \"\\n4. Make sure your GPU is CUDA-capable and supported by WSL2\")\n",
    "    \n",
    "    if not docker_configured_correctly:\n",
    "        fixes.append(\"**Docker NVIDIA Runtime Configuration**:\"\n",
    "                  \"\\n1. Install NVIDIA Container Toolkit: `sudo apt-get install -y nvidia-container-toolkit`\"\n",
    "                  \"\\n2. Configure Docker: `sudo nvidia-ctk runtime configure --runtime=docker`\"\n",
    "                  \"\\n3. Restart Docker: `sudo systemctl restart docker`\")\n",
    "    \n",
    "    if not fixes:\n",
    "        display(Markdown(\"✅ **Congratulations!** Your NVIDIA GPU appears to be properly configured for WSL and Docker.\"))\n",
    "        display(Markdown(\"You should now be able to use GPU acceleration with Ollama in the CodexContinue project.\"))\n",
    "        display(Markdown(\"Run the following to start with GPU support:\"))\n",
    "        display(Markdown(\"`docker compose -f docker-compose.yml up`\"))\n",
    "    else:\n",
    "        for i, fix in enumerate(fixes):\n",
    "            display(Markdown(f\"#### Fix {i+1}:\\n{fix}\"))\n",
    "    \n",
    "    # Add CodexContinue specific notes\n",
    "    display(Markdown(\"### CodexContinue Project Notes:\"))\n",
    "    display(Markdown(\"1. After fixing GPU issues, test the setup with Ollama:\"))\n",
    "    display(Markdown(\"   ```bash\\n   ./scripts/start-ollama-wsl.sh\\n   ```\"))\n",
    "    display(Markdown(\"2. If problems persist in CodexContinue, use the CPU-only configuration:\"))\n",
    "    display(Markdown(\"   ```bash\\n   docker compose -f docker-compose.yml -f docker-compose.macos.yml up\\n   ```\"))\n",
    "\n",
    "provide_fixes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a266dd3",
   "metadata": {},
   "source": [
    "## Troubleshooting Ollama with GPU Support\n",
    "\n",
    "Ollama can be run in different ways with GPU support in WSL. Let's explore the options and common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ollama_processes():\n",
    "    display(Markdown(\"### Checking for running Ollama processes:\"))\n",
    "    \n",
    "    # Check if Ollama is running as a standalone process\n",
    "    try:\n",
    "        result = subprocess.run(['pgrep', '-f', 'ollama'], capture_output=True, text=True, check=False)\n",
    "        if result.stdout.strip():\n",
    "            pids = result.stdout.strip().split('\\n')\n",
    "            display(Markdown(f\"⚠️ Found Ollama running as standalone process with PID(s): {', '.join(pids)}\"))\n",
    "            \n",
    "            # Check if it's listening on port 11434\n",
    "            port_check = subprocess.run(\n",
    "                ['sudo', 'lsof', '-i', ':11434'], \n",
    "                capture_output=True, text=True, check=False\n",
    "            )\n",
    "            if port_check.stdout and 'ollama' in port_check.stdout:\n",
    "                display(Markdown(\"⚠️ Ollama is using port 11434, which will conflict with Docker containers\"))\n",
    "                display(Markdown(\"To stop the standalone Ollama service:\"))\n",
    "                display(Markdown(\"```bash\\nsudo pkill -f ollama\\n```\"))\n",
    "            return True\n",
    "        else:\n",
    "            display(Markdown(\"✅ No standalone Ollama processes found\"))\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error checking Ollama processes: {str(e)}\"))\n",
    "        return False\n",
    "    \n",
    "    # Check for Docker containers running Ollama\n",
    "    try:\n",
    "        docker_check = subprocess.run(\n",
    "            ['docker', 'ps', '--filter', 'name=ollama'], \n",
    "            capture_output=True, text=True, check=False\n",
    "        )\n",
    "        if 'ollama' in docker_check.stdout:\n",
    "            display(Markdown(\"⚠️ Found Ollama running in Docker container\"))\n",
    "            display(Markdown(\"```\\n\" + docker_check.stdout + \"\\n```\"))\n",
    "            display(Markdown(\"To stop Ollama Docker containers:\"))\n",
    "            display(Markdown(\"```bash\\ndocker stop $(docker ps -q --filter name=ollama)\\n```\"))\n",
    "            return True\n",
    "        else:\n",
    "            display(Markdown(\"✅ No Ollama Docker containers found\"))\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error checking Docker: {str(e)}\"))\n",
    "        return False\n",
    "\n",
    "ollama_running = check_ollama_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190cc09",
   "metadata": {},
   "source": [
    "### Ollama GPU Support Options\n",
    "\n",
    "There are several ways to run Ollama with GPU support in WSL:\n",
    "\n",
    "1. **Standalone Ollama with GPU** - Install Ollama directly in WSL\n",
    "2. **Docker Ollama with GPU** - Run Ollama in a Docker container with GPU passthrough\n",
    "3. **Docker Compose (CodexContinue)** - Use the project's Docker Compose configuration\n",
    "\n",
    "Each approach has advantages, but you should only use one at a time to avoid port conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceab89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ollama_installation_options():\n",
    "    display(Markdown(\"### Option 1: Standalone Ollama with GPU support\"))\n",
    "    display(Markdown(\"```bash\\n# Install Ollama directly in WSL\\ncurl -fsSL https://ollama.com/install.sh | sh\\n\\n# Start the Ollama service\\nollama serve\\n```\"))\n",
    "    \n",
    "    display(Markdown(\"### Option 2: Docker Ollama with GPU support\"))\n",
    "    display(Markdown(\"```bash\\n# Run Ollama in Docker with GPU support\\ndocker run --rm -it --gpus=all -p 11434:11434 -v ollama:/root/.ollama ollama/ollama\\n```\"))\n",
    "    \n",
    "    display(Markdown(\"### Option 3: CodexContinue Docker Compose (Recommended)\"))\n",
    "    display(Markdown(\"```bash\\n# First ensure no other Ollama instances are running\\nsudo pkill -f ollama\\ndocker stop $(docker ps -q --filter name=ollama)\\n\\n# Then start CodexContinue with GPU support\\n./scripts/start-ollama-wsl.sh\\n# Or use Docker Compose directly\\ndocker compose -f docker-compose.yml up\\n```\"))\n",
    "\n",
    "show_ollama_installation_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255f0e0",
   "metadata": {},
   "source": [
    "### Resolving Port Conflicts\n",
    "\n",
    "The most common issue when starting Ollama is port conflicts. If you see an error like:\n",
    "\n",
    "```\n",
    "Error response from daemon: failed to set up container networking: driver failed programming external connectivity on endpoint codexcontinue-ollama-1: Bind for 0.0.0.0:11434 failed: port is already allocated\n",
    "```\n",
    "\n",
    "This means another process is already using port 11434. Follow these steps to resolve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_port_conflict_fix():\n",
    "    display(Markdown(\"### Fix for Port Conflicts\"))\n",
    "    \n",
    "    # Create a function to detect what's using port 11434\n",
    "    display(Markdown(\"Run these commands to identify and stop what's using port 11434:\"))\n",
    "    \n",
    "    commands = [\n",
    "        \"# Find processes using port 11434\",\n",
    "        \"sudo lsof -i :11434\",\n",
    "        \"\",\n",
    "        \"# Stop standalone Ollama if it's running\",\n",
    "        \"sudo pkill -f ollama\",\n",
    "        \"\",\n",
    "        \"# Stop any Docker containers using the port\",\n",
    "        \"docker stop $(docker ps -q --filter publish=11434)\",\n",
    "        \"\",\n",
    "        \"# Verify the port is now free\",\n",
    "        \"sudo lsof -i :11434\",\n",
    "        \"\",\n",
    "        \"# Now try starting Ollama again\",\n",
    "        \"./scripts/start-ollama-wsl.sh\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"```bash\\n\" + \"\\n\".join(commands) + \"\\n```\"))\n",
    "    \n",
    "    # Create a manage-ollama-process.sh script suggestion\n",
    "    display(Markdown(\"#### Helper Script\"))\n",
    "    display(Markdown(\"Here's a script you can create to manage Ollama processes:\"))\n",
    "    \n",
    "    script_content = [\n",
    "        \"#!/bin/bash\",\n",
    "        \"\",\n",
    "        \"function stop_ollama() {\",\n",
    "        \"  echo \\\"Stopping any running Ollama processes...\\\"\",\n",
    "        \"  sudo pkill -f ollama 2>/dev/null\",\n",
    "        \"  docker stop $(docker ps -q --filter name=ollama) 2>/dev/null\",\n",
    "        \"  docker stop $(docker ps -q --filter publish=11434) 2>/dev/null\",\n",
    "        \"  echo \\\"Done.\\\"\",\n",
    "        \"}\",\n",
    "        \"\",\n",
    "        \"function check_port() {\",\n",
    "        \"  echo \\\"Checking if port 11434 is in use...\\\"\",\n",
    "        \"  if sudo lsof -i :11434 2>/dev/null; then\",\n",
    "        \"    echo \\\"⚠️ Port 11434 is still in use.\\\"\",\n",
    "        \"    return 1\",\n",
    "        \"  else\",\n",
    "        \"    echo \\\"✅ Port 11434 is free.\\\"\",\n",
    "        \"    return 0\",\n",
    "        \"  fi\",\n",
    "        \"}\",\n",
    "        \"\",\n",
    "        \"function start_ollama_docker() {\",\n",
    "        \"  echo \\\"Starting Ollama with Docker and GPU support...\\\"\",\n",
    "        \"  stop_ollama\",\n",
    "        \"  if check_port; then\",\n",
    "        \"    ./scripts/start-ollama-wsl.sh\",\n",
    "        \"  else\",\n",
    "        \"    echo \\\"Cannot start Ollama until port 11434 is free.\\\"\",\n",
    "        \"  fi\",\n",
    "        \"}\",\n",
    "        \"\",\n",
    "        \"# Main script logic\",\n",
    "        \"case \\\"$1\\\" in\",\n",
    "        \"  stop)\",\n",
    "        \"    stop_ollama\",\n",
    "        \"    ;;\",\n",
    "        \"  start)\",\n",
    "        \"    start_ollama_docker\",\n",
    "        \"    ;;\",\n",
    "        \"  check)\",\n",
    "        \"    check_port\",\n",
    "        \"    ;;\",\n",
    "        \"  *)\",\n",
    "        \"    echo \\\"Usage: $0 {start|stop|check}\\\"\",\n",
    "        \"    echo \\\"  start - Stop any running Ollama processes and start with Docker + GPU\\\"\",\n",
    "        \"    echo \\\"  stop  - Stop all Ollama processes\\\"\",\n",
    "        \"    echo \\\"  check - Check if port 11434 is available\\\"\",\n",
    "        \"    exit 1\",\n",
    "        \"esac\",\n",
    "        \"\",\n",
    "        \"exit 0\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"```bash\\n\" + \"\\n\".join(script_content) + \"\\n```\"))\n",
    "    display(Markdown(\"Save this to `scripts/manage-ollama-process.sh` and make it executable with `chmod +x scripts/manage-ollama-process.sh`.\"))\n",
    "    display(Markdown(\"Then you can use it with:\"))\n",
    "    display(Markdown(\"```bash\\n./scripts/manage-ollama-process.sh stop  # Stop all Ollama processes\\n./scripts/manage-ollama-process.sh start # Start Ollama with Docker + GPU\\n```\"))\n",
    "\n",
    "show_port_conflict_fix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583828e",
   "metadata": {},
   "source": [
    "## Testing CodexContinue Application Ports\n",
    "\n",
    "When running the full CodexContinue application, several services operate on different ports. Here's how to verify that all the necessary ports are available and working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240be8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_port_availability(port):\n",
    "    \"\"\"Check if a port is available (not in use).\"\"\"\n",
    "    import socket\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    try:\n",
    "        s.bind((\"127.0.0.1\", port))\n",
    "        display(Markdown(f\"✅ Port {port} is available\"))\n",
    "        result = True\n",
    "    except socket.error:\n",
    "        display(Markdown(f\"❌ Port {port} is already in use\"))\n",
    "        result = False\n",
    "    finally:\n",
    "        s.close()\n",
    "    return result\n",
    "\n",
    "def check_service_running(url, service_name):\n",
    "    \"\"\"Check if a service is running at the given URL.\"\"\"\n",
    "    import requests\n",
    "    try:\n",
    "        response = requests.get(url, timeout=2)\n",
    "        if response.status_code < 400:\n",
    "            display(Markdown(f\"✅ {service_name} is running at {url}\"))\n",
    "            return True\n",
    "        else:\n",
    "            display(Markdown(f\"⚠️ {service_name} at {url} returned status code {response.status_code}\"))\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        display(Markdown(f\"❌ {service_name} at {url} is not accessible: {str(e)}\"))\n",
    "        return False\n",
    "\n",
    "def check_codexcontinue_ports():\n",
    "    \"\"\"Check all ports used by CodexContinue services.\"\"\"\n",
    "    display(Markdown(\"### CodexContinue Port Status\"))\n",
    "    \n",
    "    # Define the ports and services used by CodexContinue\n",
    "    services = [\n",
    "        {\"port\": 11434, \"name\": \"Ollama API\", \"url\": \"http://localhost:11434/api/tags\"},\n",
    "        {\"port\": 8000, \"name\": \"Backend API\", \"url\": \"http://localhost:8000/docs\"},\n",
    "        {\"port\": 8501, \"name\": \"Streamlit Frontend\", \"url\": \"http://localhost:8501\"},\n",
    "        {\"port\": 5000, \"name\": \"ML Service\", \"url\": \"http://localhost:5000/health\"}\n",
    "    ]\n",
    "    \n",
    "    # First check if the ports are in use\n",
    "    display(Markdown(\"#### Port Availability Check\"))\n",
    "    for service in services:\n",
    "        port_available = check_port_availability(service[\"port\"])\n",
    "        if port_available:\n",
    "            display(Markdown(f\"   - You will need to start the {service['name']} service\"))\n",
    "    \n",
    "    # Ask user if they want to check running services\n",
    "    display(Markdown(\"\\n#### Service Connection Check\"))\n",
    "    check_services = input(\"Do you want to check if services are running? (y/n): \")\n",
    "    \n",
    "    if check_services.lower() == 'y':\n",
    "        for service in services:\n",
    "            check_service_running(service[\"url\"], service[\"name\"])\n",
    "    else:\n",
    "        display(Markdown(\"Skipping service connection check\"))\n",
    "\n",
    "# Run the check function\n",
    "check_codexcontinue_ports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79649285",
   "metadata": {},
   "source": [
    "### Resolving CodexContinue Port Conflicts\n",
    "\n",
    "If you encounter port conflicts with any of the CodexContinue services, here's how to resolve them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_port_conflict_resolution():\n",
    "    display(Markdown(\"#### Common Port Conflict Solutions\"))\n",
    "    \n",
    "    # Table of ports and resolution methods\n",
    "    port_table = [\n",
    "        {\"port\": 11434, \"service\": \"Ollama API\", \"resolution\": \"Use `scripts/manage-ollama-process.sh stop` to stop any running Ollama instances\"},\n",
    "        {\"port\": 8000, \"service\": \"Backend API\", \"resolution\": \"Check for other FastAPI/Django/Flask services or modify the port in docker-compose.yml\"},\n",
    "        {\"port\": 8501, \"service\": \"Streamlit Frontend\", \"resolution\": \"Check for other Streamlit apps or modify the port in docker-compose.yml\"},\n",
    "        {\"port\": 5000, \"service\": \"ML Service\", \"resolution\": \"Check for other Flask/ML services or modify the port in docker-compose.yml\"}\n",
    "    ]\n",
    "    \n",
    "    # Create a Markdown table\n",
    "    table_md = \"| Port | Service | Resolution Method |\\n| ---- | ------- | ---------------- |\\n\"\n",
    "    for entry in port_table:\n",
    "        table_md += f\"| {entry['port']} | {entry['service']} | {entry['resolution']} |\\n\"\n",
    "    \n",
    "    display(Markdown(table_md))\n",
    "    \n",
    "    # Show how to check running processes on ports\n",
    "    display(Markdown(\"#### How to check what's using a port\"))\n",
    "    display(Markdown(\"```bash\\n# Replace PORT_NUMBER with the specific port (e.g., 8000)\\nsudo lsof -i :PORT_NUMBER\\n```\"))\n",
    "    \n",
    "    # Show how to modify ports in docker-compose\n",
    "    display(Markdown(\"#### How to modify service ports in docker-compose.yml\"))\n",
    "    docker_compose_example = \"\"\"\n",
    "```yaml\n",
    "services:\n",
    "  backend:\n",
    "    ports:\n",
    "      - \"8000:8000\"  # Change the first number to modify the host port\n",
    "  \n",
    "  frontend:\n",
    "    ports:\n",
    "      - \"8501:8501\"  # Change the first number to modify the host port\n",
    "  \n",
    "  ml-service:\n",
    "    ports:\n",
    "      - \"5000:5000\"  # Change the first number to modify the host port\n",
    "```\n",
    "    \"\"\"\n",
    "    display(Markdown(docker_compose_example))\n",
    "    \n",
    "    # Provide a script to test all services\n",
    "    display(Markdown(\"#### Testing Script\"))\n",
    "    display(Markdown(\"Here's a bash script you can use to test if all CodexContinue services are running:\"))\n",
    "    test_script = \"\"\"\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# test-codexcontinue-ports.sh - Test all CodexContinue services\n",
    "\n",
    "echo \"=== Testing CodexContinue Services ===\"\n",
    "\n",
    "# Define text colors\n",
    "GREEN=\"\\033[0;32m\"\n",
    "RED=\"\\033[0;31m\"\n",
    "YELLOW=\"\\033[0;33m\"\n",
    "NC=\"\\033[0m\" # No Color\n",
    "\n",
    "# Function to check a service\n",
    "check_service() {\n",
    "  local url=$1\n",
    "  local name=$2\n",
    "  echo -n \"Checking $name at $url: \"\n",
    "  if curl -s --head --request GET $url | grep -q \"HTTP/\";\n",
    "  then\n",
    "    echo -e \"${GREEN}OK${NC}\"\n",
    "    return 0\n",
    "  else\n",
    "    echo -e \"${RED}Failed${NC}\"\n",
    "    return 1\n",
    "  fi\n",
    "}\n",
    "\n",
    "# Check Ollama\n",
    "echo \"\\nTesting Ollama API...\"\n",
    "if check_service \"http://localhost:11434/api/tags\" \"Ollama API\"; then\n",
    "  # Try a quick model query\n",
    "  echo \"Testing Ollama model response...\"\n",
    "  curl -s -X POST http://localhost:11434/api/generate -d '{\"model\":\"llama3\",\"prompt\":\"hello\",\"stream\":false}' | grep -q 'response' && echo -e \"${GREEN}Model response OK${NC}\" || echo -e \"${RED}Model response failed${NC}\"\n",
    "fi\n",
    "\n",
    "# Check Backend API\n",
    "echo \"\\nTesting Backend API...\"\n",
    "check_service \"http://localhost:8000/docs\" \"API Documentation\"\n",
    "\n",
    "# Check Streamlit Frontend\n",
    "echo \"\\nTesting Streamlit Frontend...\"\n",
    "check_service \"http://localhost:8501\" \"Web Interface\"\n",
    "\n",
    "# Check ML Service\n",
    "echo \"\\nTesting ML Service...\"\n",
    "check_service \"http://localhost:5000/health\" \"ML Service Health\"\n",
    "\n",
    "# Final summary\n",
    "echo \"\\n=== Test Summary ===\"\n",
    "echo \"To start all services: ./scripts/start-codexcontinue.sh\"\n",
    "echo \"To check Ollama: ./scripts/manage-ollama-process.sh status\"\n",
    "```\n",
    "    \"\"\"\n",
    "    display(Markdown(test_script))\n",
    "\n",
    "show_port_conflict_resolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208e3cc",
   "metadata": {},
   "source": [
    "### Creating a Port Testing Script for CodexContinue\n",
    "\n",
    "Let's create a reusable bash script for testing all CodexContinue service ports that you can run anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_port_testing_script():\n",
    "    # Create the script content\n",
    "    script_content = \"\"\"#!/bin/bash\n",
    "# test-codexcontinue-ports.sh - Test all CodexContinue service ports\n",
    "\n",
    "# Colors for output\n",
    "GREEN='\\033[0;32m'\n",
    "RED='\\033[0;31m'\n",
    "YELLOW='\\033[0;33m'\n",
    "BLUE='\\033[0;34m'\n",
    "NC='\\033[0m' # No Color\n",
    "\n",
    "# Helper functions\n",
    "log() {\n",
    "    echo -e \"${GREEN}[INFO]${NC} $1\"\n",
    "}\n",
    "\n",
    "warn() {\n",
    "    echo -e \"${YELLOW}[WARNING]${NC} $1\"\n",
    "}\n",
    "\n",
    "error() {\n",
    "    echo -e \"${RED}[ERROR]${NC} $1\"\n",
    "}\n",
    "\n",
    "section() {\n",
    "    echo -e \"\\n${BLUE}=== $1 ===${NC}\"\n",
    "}\n",
    "\n",
    "# Make sure we're in the project root\n",
    "cd \"$(dirname \\\"$0\\\")/..\"\n",
    "\n",
    "echo \"===============================================\"\n",
    "echo \"CodexContinue Service Port Test\"\n",
    "echo \"===============================================\"\n",
    "echo \"This script tests all service ports used by CodexContinue\"\n",
    "echo\n",
    "\n",
    "# Define the services and their ports\n",
    "declare -A services=(\n",
    "    [\"Ollama API\"]=11434\n",
    "    [\"Backend API\"]=8000\n",
    "    [\"Streamlit Frontend\"]=8501\n",
    "    [\"ML Service\"]=5000\n",
    ")\n",
    "\n",
    "# Function to check if a port is in use\n",
    "check_port_in_use() {\n",
    "    local port=$1\n",
    "    local service_name=$2\n",
    "    \n",
    "    if lsof -i :\"$port\" &>/dev/null; then\n",
    "        log \"Port $port ($service_name) is in use\"\n",
    "        return 0 # true, port is in use\n",
    "    else\n",
    "        warn \"Port $port ($service_name) is not in use - service may not be running\"\n",
    "        return 1 # false, port is not in use\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Function to check if a service is responding\n",
    "check_service_responding() {\n",
    "    local port=$1\n",
    "    local service_name=$2\n",
    "    local endpoint=$3\n",
    "    \n",
    "    # Create URL from port and endpoint\n",
    "    local url=\"http://localhost:${port}${endpoint}\"\n",
    "    \n",
    "    echo -n \"  Testing $service_name at $url: \"\n",
    "    \n",
    "    # Try to connect to the service\n",
    "    if curl -s --head --request GET \"$url\" -m 2 | grep -q \"HTTP/\";\n",
    "    then\n",
    "        echo -e \"${GREEN}OK${NC}\"\n",
    "        return 0\n",
    "    else\n",
    "        echo -e \"${RED}Failed${NC}\"\n",
    "        return 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Check all port usage first\n",
    "section \"Checking Port Usage\"\n",
    "for service in \"${!services[@]}\"; do\n",
    "    port=${services[$service]}\n",
    "    check_port_in_use \"$port\" \"$service\"\n",
    "done\n",
    "\n",
    "# Now check service responses\n",
    "section \"Testing Service Responses\"\n",
    "\n",
    "# Ollama API\n",
    "if check_port_in_use \"${services['Ollama API']}\" \"Ollama API\"; then\n",
    "    check_service_responding \"${services['Ollama API']}\" \"Ollama API\" \"/api/tags\"\n",
    "    \n",
    "    # Try a quick model query if available\n",
    "    echo \"  Testing Ollama model response...\"\n",
    "    if curl -s -X POST http://localhost:11434/api/generate -d '{\"model\":\"llama3\",\"prompt\":\"hello\",\"stream\":false}' | grep -q 'response'; then\n",
    "        log \"Ollama model response: OK\"\n",
    "    else\n",
    "        warn \"Ollama model response failed - models may not be loaded\"\n",
    "    fi\n",
    "fi\n",
    "\n",
    "# Backend API\n",
    "if check_port_in_use \"${services['Backend API']}\" \"Backend API\"; then\n",
    "    check_service_responding \"${services['Backend API']}\" \"Backend API\" \"/docs\"\n",
    "fi\n",
    "\n",
    "# Streamlit Frontend\n",
    "if check_port_in_use \"${services['Streamlit Frontend']}\" \"Streamlit Frontend\"; then\n",
    "    check_service_responding \"${services['Streamlit Frontend']}\" \"Streamlit Frontend\" \"/\"\n",
    "fi\n",
    "\n",
    "# ML Service\n",
    "if check_port_in_use \"${services['ML Service']}\" \"ML Service\"; then\n",
    "    check_service_responding \"${services['ML Service']}\" \"ML Service\" \"/health\"\n",
    "fi\n",
    "\n",
    "# Final summary and tips\n",
    "section \"Summary and Tips\"\n",
    "echo \"If any services are not running, try the following:\"\n",
    "echo\n",
    "echo \"1. To start all CodexContinue services:\"\n",
    "echo \"   ./scripts/start-codexcontinue.sh\"\n",
    "echo\n",
    "echo \"2. To manage Ollama separately:\"\n",
    "echo \"   ./scripts/manage-ollama-process.sh stop    # Stop Ollama\"\n",
    "echo \"   ./scripts/manage-ollama-process.sh start   # Start Ollama\"\n",
    "echo \"   ./scripts/manage-ollama-process.sh status  # Check Ollama status\"\n",
    "echo\n",
    "echo \"3. For port conflicts, check what processes are using the ports:\"\n",
    "echo \"   sudo lsof -i :PORT_NUMBER  # Replace PORT_NUMBER with the port\"\n",
    "echo\n",
    "echo \"4. Modify ports in docker-compose.yml if needed (change first number):\"\n",
    "echo \"   ports:\"\n",
    "echo \"     - \\\"NEW_PORT:ORIGINAL_PORT\\\"\"\n",
    "\n",
    "echo\n",
    "echo \"For more information, see docs/troubleshooting/\"\n",
    "\"\"\"\n",
    "    \n",
    "    # Ask for confirmation\n",
    "    create_script = input(\"Do you want to create the port testing script in the scripts directory? (y/n): \")\n",
    "    \n",
    "    if create_script.lower() == 'y':\n",
    "        # Define the path\n",
    "        script_path = \"../scripts/test-codexcontinue-ports.sh\"\n",
    "        \n",
    "        try:\n",
    "            with open(script_path, 'w') as f:\n",
    "                f.write(script_content)\n",
    "            \n",
    "            # Make the script executable\n",
    "            import os\n",
    "            os.chmod(script_path, 0o755)\n",
    "            \n",
    "            display(Markdown(f\"✅ Created port testing script at: {script_path}\"))\n",
    "            display(Markdown(\"To use it, run:\\n```bash\\n./scripts/test-codexcontinue-ports.sh\\n```\"))\n",
    "            \n",
    "            # Suggest adding to git\n",
    "            display(Markdown(\"Don't forget to add it to git:\\n```bash\\ngit add scripts/test-codexcontinue-ports.sh\\ngit commit -m \"Add port testing script for CodexContinue services\"\\n```\"))\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"❌ Error creating script: {str(e)}\"))\n",
    "            return False\n",
    "    else:\n",
    "        display(Markdown(\"Script creation skipped.\"))\n",
    "        # Show the script content instead\n",
    "        display(Markdown(\"Here's the script content if you want to create it manually later:\"))\n",
    "        display(Markdown(\"```bash\\n\" + script_content + \"\\n```\"))\n",
    "        return False\n",
    "\n",
    "create_port_testing_script()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

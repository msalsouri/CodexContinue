{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a99893",
   "metadata": {},
   "source": [
    "# NVIDIA GPU Setup Guide for WSL in CodexContinue\n",
    "\n",
    "This notebook provides a step-by-step guide for diagnosing and fixing NVIDIA GPU support in Windows Subsystem for Linux (WSL) for the CodexContinue project. The guide focuses on resolving common issues with missing libraries and ensuring proper Docker integration with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bd8c7",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import the necessary Python libraries for system checks and diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77348767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For prettier output\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a50f7",
   "metadata": {},
   "source": [
    "## Check NVIDIA Libraries in WSL\n",
    "\n",
    "Let's check for the presence of NVIDIA libraries in the standard WSL location (`/usr/lib/wsl/lib/libnvidia-ml.so`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nvidia_libraries():\n",
    "    wsl_lib_path = \"/usr/lib/wsl/lib/libnvidia-ml.so\"\n",
    "    if os.path.exists(wsl_lib_path):\n",
    "        display(Markdown(f\"✅ Found NVIDIA libraries in WSL driver location: `{wsl_lib_path}`\"))\n",
    "    else:\n",
    "        display(Markdown(f\"❌ Missing NVIDIA libraries in standard WSL location: `{wsl_lib_path}`\"))\n",
    "        \n",
    "    # Check LD_LIBRARY_PATH\n",
    "    ld_library_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "    display(Markdown(f\"Current LD_LIBRARY_PATH: `{ld_library_path}`\"))\n",
    "\n",
    "check_nvidia_libraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bdc71",
   "metadata": {},
   "source": [
    "## Verify `nvidia-smi` Output\n",
    "\n",
    "Now, let's run the `nvidia-smi` command to verify GPU status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nvidia_smi():\n",
    "    try:\n",
    "        result = subprocess.run(['which', 'nvidia-smi'], capture_output=True, text=True, check=False)\n",
    "        if result.returncode == 0:\n",
    "            display(Markdown(f\"Found nvidia-smi at: `{result.stdout.strip()}`\"))\n",
    "            \n",
    "            # Run nvidia-smi\n",
    "            nvidia_smi_output = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=False)\n",
    "            if nvidia_smi_output.returncode == 0:\n",
    "                display(Markdown(\"### nvidia-smi output:\"))\n",
    "                print(nvidia_smi_output.stdout)\n",
    "                return True\n",
    "            else:\n",
    "                display(Markdown(f\"❌ Error running nvidia-smi: {nvidia_smi_output.stderr}\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ nvidia-smi command not found\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ Error: {str(e)}\"))\n",
    "    return False\n",
    "\n",
    "ran_successfully = run_nvidia_smi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9394cfd",
   "metadata": {},
   "source": [
    "## Search for NVIDIA Libraries\n",
    "\n",
    "Let's search for `libnvidia-ml.so` in common library paths and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nvidia_libraries():\n",
    "    display(Markdown(\"Looking for libnvidia-ml.so in common paths:\"))\n",
    "    \n",
    "    search_paths = [\n",
    "        \"/usr/lib\",\n",
    "        \"/usr/lib/x86_64-linux-gnu\",\n",
    "        \"/usr/lib/wsl/lib\",\n",
    "        \"/usr/local/cuda*/targets/x86_64-linux/lib\"\n",
    "    ]\n",
    "    \n",
    "    found_libs = []\n",
    "    \n",
    "    for path in search_paths:\n",
    "        # Handle glob patterns\n",
    "        if '*' in path:\n",
    "            matching_dirs = glob.glob(path)\n",
    "            for dir in matching_dirs:\n",
    "                lib_file = os.path.join(dir, \"libnvidia-ml.so\")\n",
    "                if os.path.exists(lib_file):\n",
    "                    found_libs.append(lib_file)\n",
    "                # Check for stub files\n",
    "                stub_path = os.path.join(dir, \"stubs/libnvidia-ml.so\")\n",
    "                if os.path.exists(stub_path):\n",
    "                    found_libs.append(stub_path + \" (stub)\")\n",
    "        else:\n",
    "            lib_file = os.path.join(path, \"libnvidia-ml.so\")\n",
    "            if os.path.exists(lib_file):\n",
    "                found_libs.append(lib_file)\n",
    "    \n",
    "    if found_libs:\n",
    "        for lib in found_libs:\n",
    "            display(Markdown(f\"- Found `{lib}`\"))\n",
    "    else:\n",
    "        display(Markdown(\"❌ No NVIDIA libraries found in common paths\"))\n",
    "\n",
    "search_nvidia_libraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008d2e5",
   "metadata": {},
   "source": [
    "## Check NVIDIA Container Toolkit Configuration\n",
    "\n",
    "Let's read and parse the Docker daemon configuration file to verify NVIDIA runtime settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_docker_nvidia_config():\n",
    "    docker_config_path = \"/etc/docker/daemon.json\"\n",
    "    \n",
    "    if os.path.exists(docker_config_path):\n",
    "        try:\n",
    "            with open(docker_config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "            display(Markdown(\"### Docker daemon configuration:\"))\n",
    "            print(json.dumps(config, indent=4))\n",
    "            \n",
    "            if 'runtimes' in config and 'nvidia' in config['runtimes']:\n",
    "                display(Markdown(\"✅ NVIDIA Container Runtime is configured\"))\n",
    "                return True\n",
    "            else:\n",
    "                display(Markdown(\"❌ NVIDIA Container Runtime is not configured\"))\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"Error reading Docker configuration: {str(e)}\"))\n",
    "    else:\n",
    "        display(Markdown(f\"❌ Docker daemon configuration file not found at {docker_config_path}\"))\n",
    "    \n",
    "    return False\n",
    "\n",
    "docker_configured = check_docker_nvidia_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a0f45",
   "metadata": {},
   "source": [
    "## Test Docker GPU Capability\n",
    "\n",
    "Now let's run a Docker command to check for GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ddb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_docker_gpu():\n",
    "    display(Markdown(\"### Testing Docker with GPU support:\"))\n",
    "    \n",
    "    # First check docker info for nvidia runtime\n",
    "    try:\n",
    "        docker_info = subprocess.run(['docker', 'info'], capture_output=True, text=True, check=False)\n",
    "        if 'nvidia' in docker_info.stdout and 'Runtimes' in docker_info.stdout:\n",
    "            display(Markdown(\"✅ Docker info shows nvidia runtime is available\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ Docker info does not show nvidia runtime\"))\n",
    "            print(\"Docker runtimes found:\")\n",
    "            for line in docker_info.stdout.splitlines():\n",
    "                if 'Runtimes' in line:\n",
    "                    print(line)\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error checking Docker info: {str(e)}\"))\n",
    "    \n",
    "    # Try running a container with GPU access\n",
    "    try:\n",
    "        display(Markdown(\"### Testing GPU access with a CUDA container:\"))\n",
    "        display(Markdown(\"Running: `docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi`\"))\n",
    "        \n",
    "        # Run the command but don't capture output as it might be large\n",
    "        display(Markdown(\"This might take a moment to pull the container image if it's not already cached...\"))\n",
    "        result = subprocess.run(\n",
    "            ['docker', 'run', '--rm', '--gpus', 'all', 'nvidia/cuda:11.6.2-base-ubuntu20.04', 'nvidia-smi'],\n",
    "            capture_output=True, text=True, check=False\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            display(Markdown(\"✅ Successfully ran nvidia-smi in a Docker container with GPU access\"))\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            display(Markdown(f\"❌ Failed to run nvidia-smi in Docker: {result.stderr}\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error testing Docker GPU support: {str(e)}\"))\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Only run if the user wants to test Docker\n",
    "test_docker = input(\"Do you want to test Docker with GPU support? (y/n): \")\n",
    "if test_docker.lower() == 'y':\n",
    "    docker_test_successful = test_docker_gpu()\n",
    "else:\n",
    "    display(Markdown(\"Skipping Docker GPU test\"))\n",
    "    docker_test_successful = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9120e8",
   "metadata": {},
   "source": [
    "## Provide Fix Suggestions\n",
    "\n",
    "Based on the diagnostic results, here are actionable suggestions to fix common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_fixes():\n",
    "    display(Markdown(\"## Diagnostic Summary and Fix Suggestions\"))\n",
    "    \n",
    "    # Check if NVIDIA libraries exist in /usr/lib/wsl/lib\n",
    "    wsl_lib_path = \"/usr/lib/wsl/lib/libnvidia-ml.so\"\n",
    "    wsl_lib_exists = os.path.exists(wsl_lib_path)\n",
    "    \n",
    "    # Check if we found nvidia-smi earlier\n",
    "    nvidia_smi_runs = ran_successfully\n",
    "    \n",
    "    # Check Docker configuration\n",
    "    docker_configured_correctly = docker_configured\n",
    "    \n",
    "    # Print summary\n",
    "    display(Markdown(\"### Status Summary:\"))\n",
    "    display(Markdown(f\"- NVIDIA libraries in WSL path: {'✅ Found' if wsl_lib_exists else '❌ Missing'}\"))\n",
    "    display(Markdown(f\"- nvidia-smi runs successfully: {'✅ Yes' if nvidia_smi_runs else '❌ No'}\"))\n",
    "    display(Markdown(f\"- Docker NVIDIA runtime configured: {'✅ Yes' if docker_configured_correctly else '❌ No'}\"))\n",
    "    \n",
    "    # Generate fix suggestions\n",
    "    display(Markdown(\"### Fix Suggestions:\"))\n",
    "    \n",
    "    fixes = []\n",
    "    \n",
    "    if not wsl_lib_exists and nvidia_smi_runs:\n",
    "        # We have NVIDIA drivers but missing the symbolic link\n",
    "        fixes.append(\"**Missing Library Symbolic Link**:\"\n",
    "                  \"\\n1. Run the fix script: `sudo ./scripts/fix-nvidia-wsl-libs.sh`\"\n",
    "                  \"\\n2. This will create the symbolic link from an existing library to the standard WSL location\")\n",
    "    \n",
    "    if not nvidia_smi_runs:\n",
    "        fixes.append(\"**NVIDIA Driver Issues**:\"\n",
    "                  \"\\n1. Verify you have the NVIDIA driver for WSL installed on Windows (not in WSL)\"\n",
    "                  \"\\n2. Install from: https://developer.nvidia.com/cuda/wsl\"\n",
    "                  \"\\n3. Restart your Windows system after installation\"\n",
    "                  \"\\n4. Make sure your GPU is CUDA-capable and supported by WSL2\")\n",
    "    \n",
    "    if not docker_configured_correctly:\n",
    "        fixes.append(\"**Docker NVIDIA Runtime Configuration**:\"\n",
    "                  \"\\n1. Install NVIDIA Container Toolkit: `sudo apt-get install -y nvidia-container-toolkit`\"\n",
    "                  \"\\n2. Configure Docker: `sudo nvidia-ctk runtime configure --runtime=docker`\"\n",
    "                  \"\\n3. Restart Docker: `sudo systemctl restart docker`\")\n",
    "    \n",
    "    if not fixes:\n",
    "        display(Markdown(\"✅ **Congratulations!** Your NVIDIA GPU appears to be properly configured for WSL and Docker.\"))\n",
    "        display(Markdown(\"You should now be able to use GPU acceleration with Ollama in the CodexContinue project.\"))\n",
    "        display(Markdown(\"Run the following to start with GPU support:\"))\n",
    "        display(Markdown(\"`docker compose -f docker-compose.yml up`\"))\n",
    "    else:\n",
    "        for i, fix in enumerate(fixes):\n",
    "            display(Markdown(f\"#### Fix {i+1}:\\n{fix}\"))\n",
    "    \n",
    "    # Add CodexContinue specific notes\n",
    "    display(Markdown(\"### CodexContinue Project Notes:\"))\n",
    "    display(Markdown(\"1. After fixing GPU issues, test the setup with Ollama:\"))\n",
    "    display(Markdown(\"   ```bash\\n   ./scripts/start-ollama-wsl.sh\\n   ```\"))\n",
    "    display(Markdown(\"2. If problems persist in CodexContinue, use the CPU-only configuration:\"))\n",
    "    display(Markdown(\"   ```bash\\n   docker compose -f docker-compose.yml -f docker-compose.macos.yml up\\n   ```\"))\n",
    "\n",
    "provide_fixes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a266dd3",
   "metadata": {},
   "source": [
    "## Troubleshooting Ollama with GPU Support\n",
    "\n",
    "Ollama can be run in different ways with GPU support in WSL. Let's explore the options and common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ollama_processes():\n",
    "    display(Markdown(\"### Checking for running Ollama processes:\"))\n",
    "    \n",
    "    # Check if Ollama is running as a standalone process\n",
    "    try:\n",
    "        result = subprocess.run(['pgrep', '-f', 'ollama'], capture_output=True, text=True, check=False)\n",
    "        if result.stdout.strip():\n",
    "            pids = result.stdout.strip().split('\\n')\n",
    "            display(Markdown(f\"⚠️ Found Ollama running as standalone process with PID(s): {', '.join(pids)}\"))\n",
    "            \n",
    "            # Check if it's listening on port 11434\n",
    "            port_check = subprocess.run(\n",
    "                ['sudo', 'lsof', '-i', ':11434'], \n",
    "                capture_output=True, text=True, check=False\n",
    "            )\n",
    "            if port_check.stdout and 'ollama' in port_check.stdout:\n",
    "                display(Markdown(\"⚠️ Ollama is using port 11434, which will conflict with Docker containers\"))\n",
    "                display(Markdown(\"To stop the standalone Ollama service:\"))\n",
    "                display(Markdown(\"```bash\\nsudo pkill -f ollama\\n```\"))\n",
    "            return True\n",
    "        else:\n",
    "            display(Markdown(\"✅ No standalone Ollama processes found\"))\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error checking Ollama processes: {str(e)}\"))\n",
    "        return False\n",
    "    \n",
    "    # Check for Docker containers running Ollama\n",
    "    try:\n",
    "        docker_check = subprocess.run(\n",
    "            ['docker', 'ps', '--filter', 'name=ollama'], \n",
    "            capture_output=True, text=True, check=False\n",
    "        )\n",
    "        if 'ollama' in docker_check.stdout:\n",
    "            display(Markdown(\"⚠️ Found Ollama running in Docker container\"))\n",
    "            display(Markdown(\"```\\n\" + docker_check.stdout + \"\\n```\"))\n",
    "            display(Markdown(\"To stop Ollama Docker containers:\"))\n",
    "            display(Markdown(\"```bash\\ndocker stop $(docker ps -q --filter name=ollama)\\n```\"))\n",
    "            return True\n",
    "        else:\n",
    "            display(Markdown(\"✅ No Ollama Docker containers found\"))\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error checking Docker: {str(e)}\"))\n",
    "        return False\n",
    "\n",
    "ollama_running = check_ollama_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190cc09",
   "metadata": {},
   "source": [
    "### Ollama GPU Support Options\n",
    "\n",
    "There are several ways to run Ollama with GPU support in WSL:\n",
    "\n",
    "1. **Standalone Ollama with GPU** - Install Ollama directly in WSL\n",
    "2. **Docker Ollama with GPU** - Run Ollama in a Docker container with GPU passthrough\n",
    "3. **Docker Compose (CodexContinue)** - Use the project's Docker Compose configuration\n",
    "\n",
    "Each approach has advantages, but you should only use one at a time to avoid port conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceab89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ollama_installation_options():\n",
    "    display(Markdown(\"### Option 1: Standalone Ollama with GPU support\"))\n",
    "    display(Markdown(\"```bash\\n# Install Ollama directly in WSL\\ncurl -fsSL https://ollama.com/install.sh | sh\\n\\n# Start the Ollama service\\nollama serve\\n```\"))\n",
    "    \n",
    "    display(Markdown(\"### Option 2: Docker Ollama with GPU support\"))\n",
    "    display(Markdown(\"```bash\\n# Run Ollama in Docker with GPU support\\ndocker run --rm -it --gpus=all -p 11434:11434 -v ollama:/root/.ollama ollama/ollama\\n```\"))\n",
    "    \n",
    "    display(Markdown(\"### Option 3: CodexContinue Docker Compose (Recommended)\"))\n",
    "    display(Markdown(\"```bash\\n# First ensure no other Ollama instances are running\\nsudo pkill -f ollama\\ndocker stop $(docker ps -q --filter name=ollama)\\n\\n# Then start CodexContinue with GPU support\\n./scripts/start-ollama-wsl.sh\\n# Or use Docker Compose directly\\ndocker compose -f docker-compose.yml up\\n```\"))\n",
    "\n",
    "show_ollama_installation_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255f0e0",
   "metadata": {},
   "source": [
    "### Resolving Port Conflicts\n",
    "\n",
    "The most common issue when starting Ollama is port conflicts. If you see an error like:\n",
    "\n",
    "```\n",
    "Error response from daemon: failed to set up container networking: driver failed programming external connectivity on endpoint codexcontinue-ollama-1: Bind for 0.0.0.0:11434 failed: port is already allocated\n",
    "```\n",
    "\n",
    "This means another process is already using port 11434. Follow these steps to resolve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_port_conflict_fix():\n",
    "    display(Markdown(\"### Fix for Port Conflicts\"))\n",
    "    \n",
    "    # Create a function to detect what's using port 11434\n",
    "    display(Markdown(\"Run these commands to identify and stop what's using port 11434:\"))\n",
    "    \n",
    "    commands = [\n",
    "        \"# Find processes using port 11434\",\n",
    "        \"sudo lsof -i :11434\",\n",
    "        \"\",\n",
    "        \"# Stop standalone Ollama if it's running\",\n",
    "        \"sudo pkill -f ollama\",\n",
    "        \"\",\n",
    "        \"# Stop any Docker containers using the port\",\n",
    "        \"docker stop $(docker ps -q --filter publish=11434)\",\n",
    "        \"\",\n",
    "        \"# Verify the port is now free\",\n",
    "        \"sudo lsof -i :11434\",\n",
    "        \"\",\n",
    "        \"# Now try starting Ollama again\",\n",
    "        \"./scripts/start-ollama-wsl.sh\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"```bash\\n\" + \"\\n\".join(commands) + \"\\n```\"))\n",
    "    \n",
    "    # Create a manage-ollama-process.sh script suggestion\n",
    "    display(Markdown(\"#### Helper Script\"))\n",
    "    display(Markdown(\"Here's a script you can create to manage Ollama processes:\"))\n",
    "    \n",
    "    script_content = [\n",
    "        \"#!/bin/bash\",\n",
    "        \"\",\n",
    "        \"function stop_ollama() {\",\n",
    "        \"  echo \\\"Stopping any running Ollama processes...\\\"\",\n",
    "        \"  sudo pkill -f ollama 2>/dev/null\",\n",
    "        \"  docker stop $(docker ps -q --filter name=ollama) 2>/dev/null\",\n",
    "        \"  docker stop $(docker ps -q --filter publish=11434) 2>/dev/null\",\n",
    "        \"  echo \\\"Done.\\\"\",\n",
    "        \"}\",\n",
    "        \"\",\n",
    "        \"function check_port() {\",\n",
    "        \"  echo \\\"Checking if port 11434 is in use...\\\"\",\n",
    "        \"  if sudo lsof -i :11434 2>/dev/null; then\",\n",
    "        \"    echo \\\"⚠️ Port 11434 is still in use.\\\"\",\n",
    "        \"    return 1\",\n",
    "        \"  else\",\n",
    "        \"    echo \\\"✅ Port 11434 is free.\\\"\",\n",
    "        \"    return 0\",\n",
    "        \"  fi\",\n",
    "        \"}\",\n",
    "        \"\",\n",
    "        \"function start_ollama_docker() {\",\n",
    "        \"  echo \\\"Starting Ollama with Docker and GPU support...\\\"\",\n",
    "        \"  stop_ollama\",\n",
    "        \"  if check_port; then\",\n",
    "        \"    ./scripts/start-ollama-wsl.sh\",\n",
    "        \"  else\",\n",
    "        \"    echo \\\"Cannot start Ollama until port 11434 is free.\\\"\",\n",
    "        \"  fi\",\n",
    "        \"}\",\n",
    "        \"\",\n",
    "        \"# Main script logic\",\n",
    "        \"case \\\"$1\\\" in\",\n",
    "        \"  stop)\",\n",
    "        \"    stop_ollama\",\n",
    "        \"    ;;\",\n",
    "        \"  start)\",\n",
    "        \"    start_ollama_docker\",\n",
    "        \"    ;;\",\n",
    "        \"  check)\",\n",
    "        \"    check_port\",\n",
    "        \"    ;;\",\n",
    "        \"  *)\",\n",
    "        \"    echo \\\"Usage: $0 {start|stop|check}\\\"\",\n",
    "        \"    echo \\\"  start - Stop any running Ollama processes and start with Docker + GPU\\\"\",\n",
    "        \"    echo \\\"  stop  - Stop all Ollama processes\\\"\",\n",
    "        \"    echo \\\"  check - Check if port 11434 is available\\\"\",\n",
    "        \"    exit 1\",\n",
    "        \"esac\",\n",
    "        \"\",\n",
    "        \"exit 0\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"```bash\\n\" + \"\\n\".join(script_content) + \"\\n```\"))\n",
    "    display(Markdown(\"Save this to `scripts/manage-ollama-process.sh` and make it executable with `chmod +x scripts/manage-ollama-process.sh`.\"))\n",
    "    display(Markdown(\"Then you can use it with:\"))\n",
    "    display(Markdown(\"```bash\\n./scripts/manage-ollama-process.sh stop  # Stop all Ollama processes\\n./scripts/manage-ollama-process.sh start # Start Ollama with Docker + GPU\\n```\"))\n",
    "\n",
    "show_port_conflict_fix()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

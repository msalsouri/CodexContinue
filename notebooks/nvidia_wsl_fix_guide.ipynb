{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a99893",
   "metadata": {},
   "source": [
    "# NVIDIA GPU Setup Guide for WSL in CodexContinue\n",
    "\n",
    "This notebook provides a step-by-step guide for diagnosing and fixing NVIDIA GPU support in Windows Subsystem for Linux (WSL) for the CodexContinue project. The guide focuses on resolving common issues with missing libraries and ensuring proper Docker integration with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bd8c7",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import the necessary Python libraries for system checks and diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77348767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For prettier output\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a50f7",
   "metadata": {},
   "source": [
    "## Check NVIDIA Libraries in WSL\n",
    "\n",
    "Let's check for the presence of NVIDIA libraries in the standard WSL location (`/usr/lib/wsl/lib/libnvidia-ml.so`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nvidia_libraries():\n",
    "    wsl_lib_path = \"/usr/lib/wsl/lib/libnvidia-ml.so\"\n",
    "    if os.path.exists(wsl_lib_path):\n",
    "        display(Markdown(f\"✅ Found NVIDIA libraries in WSL driver location: `{wsl_lib_path}`\"))\n",
    "    else:\n",
    "        display(Markdown(f\"❌ Missing NVIDIA libraries in standard WSL location: `{wsl_lib_path}`\"))\n",
    "        \n",
    "    # Check LD_LIBRARY_PATH\n",
    "    ld_library_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "    display(Markdown(f\"Current LD_LIBRARY_PATH: `{ld_library_path}`\"))\n",
    "\n",
    "check_nvidia_libraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bdc71",
   "metadata": {},
   "source": [
    "## Verify `nvidia-smi` Output\n",
    "\n",
    "Now, let's run the `nvidia-smi` command to verify GPU status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nvidia_smi():\n",
    "    try:\n",
    "        result = subprocess.run(['which', 'nvidia-smi'], capture_output=True, text=True, check=False)\n",
    "        if result.returncode == 0:\n",
    "            display(Markdown(f\"Found nvidia-smi at: `{result.stdout.strip()}`\"))\n",
    "            \n",
    "            # Run nvidia-smi\n",
    "            nvidia_smi_output = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=False)\n",
    "            if nvidia_smi_output.returncode == 0:\n",
    "                display(Markdown(\"### nvidia-smi output:\"))\n",
    "                print(nvidia_smi_output.stdout)\n",
    "                return True\n",
    "            else:\n",
    "                display(Markdown(f\"❌ Error running nvidia-smi: {nvidia_smi_output.stderr}\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ nvidia-smi command not found\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ Error: {str(e)}\"))\n",
    "    return False\n",
    "\n",
    "ran_successfully = run_nvidia_smi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9394cfd",
   "metadata": {},
   "source": [
    "## Search for NVIDIA Libraries\n",
    "\n",
    "Let's search for `libnvidia-ml.so` in common library paths and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nvidia_libraries():\n",
    "    display(Markdown(\"Looking for libnvidia-ml.so in common paths:\"))\n",
    "    \n",
    "    search_paths = [\n",
    "        \"/usr/lib\",\n",
    "        \"/usr/lib/x86_64-linux-gnu\",\n",
    "        \"/usr/lib/wsl/lib\",\n",
    "        \"/usr/local/cuda*/targets/x86_64-linux/lib\"\n",
    "    ]\n",
    "    \n",
    "    found_libs = []\n",
    "    \n",
    "    for path in search_paths:\n",
    "        # Handle glob patterns\n",
    "        if '*' in path:\n",
    "            matching_dirs = glob.glob(path)\n",
    "            for dir in matching_dirs:\n",
    "                lib_file = os.path.join(dir, \"libnvidia-ml.so\")\n",
    "                if os.path.exists(lib_file):\n",
    "                    found_libs.append(lib_file)\n",
    "                # Check for stub files\n",
    "                stub_path = os.path.join(dir, \"stubs/libnvidia-ml.so\")\n",
    "                if os.path.exists(stub_path):\n",
    "                    found_libs.append(stub_path + \" (stub)\")\n",
    "        else:\n",
    "            lib_file = os.path.join(path, \"libnvidia-ml.so\")\n",
    "            if os.path.exists(lib_file):\n",
    "                found_libs.append(lib_file)\n",
    "    \n",
    "    if found_libs:\n",
    "        for lib in found_libs:\n",
    "            display(Markdown(f\"- Found `{lib}`\"))\n",
    "    else:\n",
    "        display(Markdown(\"❌ No NVIDIA libraries found in common paths\"))\n",
    "\n",
    "search_nvidia_libraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008d2e5",
   "metadata": {},
   "source": [
    "## Check NVIDIA Container Toolkit Configuration\n",
    "\n",
    "Let's read and parse the Docker daemon configuration file to verify NVIDIA runtime settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_docker_nvidia_config():\n",
    "    docker_config_path = \"/etc/docker/daemon.json\"\n",
    "    \n",
    "    if os.path.exists(docker_config_path):\n",
    "        try:\n",
    "            with open(docker_config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "            display(Markdown(\"### Docker daemon configuration:\"))\n",
    "            print(json.dumps(config, indent=4))\n",
    "            \n",
    "            if 'runtimes' in config and 'nvidia' in config['runtimes']:\n",
    "                display(Markdown(\"✅ NVIDIA Container Runtime is configured\"))\n",
    "                return True\n",
    "            else:\n",
    "                display(Markdown(\"❌ NVIDIA Container Runtime is not configured\"))\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"Error reading Docker configuration: {str(e)}\"))\n",
    "    else:\n",
    "        display(Markdown(f\"❌ Docker daemon configuration file not found at {docker_config_path}\"))\n",
    "    \n",
    "    return False\n",
    "\n",
    "docker_configured = check_docker_nvidia_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a0f45",
   "metadata": {},
   "source": [
    "## Test Docker GPU Capability\n",
    "\n",
    "Now let's run a Docker command to check for GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ddb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_docker_gpu():\n",
    "    display(Markdown(\"### Testing Docker with GPU support:\"))\n",
    "    \n",
    "    # First check docker info for nvidia runtime\n",
    "    try:\n",
    "        docker_info = subprocess.run(['docker', 'info'], capture_output=True, text=True, check=False)\n",
    "        if 'nvidia' in docker_info.stdout and 'Runtimes' in docker_info.stdout:\n",
    "            display(Markdown(\"✅ Docker info shows nvidia runtime is available\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ Docker info does not show nvidia runtime\"))\n",
    "            print(\"Docker runtimes found:\")\n",
    "            for line in docker_info.stdout.splitlines():\n",
    "                if 'Runtimes' in line:\n",
    "                    print(line)\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error checking Docker info: {str(e)}\"))\n",
    "    \n",
    "    # Try running a container with GPU access\n",
    "    try:\n",
    "        display(Markdown(\"### Testing GPU access with a CUDA container:\"))\n",
    "        display(Markdown(\"Running: `docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi`\"))\n",
    "        \n",
    "        # Run the command but don't capture output as it might be large\n",
    "        display(Markdown(\"This might take a moment to pull the container image if it's not already cached...\"))\n",
    "        result = subprocess.run(\n",
    "            ['docker', 'run', '--rm', '--gpus', 'all', 'nvidia/cuda:11.6.2-base-ubuntu20.04', 'nvidia-smi'],\n",
    "            capture_output=True, text=True, check=False\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            display(Markdown(\"✅ Successfully ran nvidia-smi in a Docker container with GPU access\"))\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            display(Markdown(f\"❌ Failed to run nvidia-smi in Docker: {result.stderr}\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error testing Docker GPU support: {str(e)}\"))\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Only run if the user wants to test Docker\n",
    "test_docker = input(\"Do you want to test Docker with GPU support? (y/n): \")\n",
    "if test_docker.lower() == 'y':\n",
    "    docker_test_successful = test_docker_gpu()\n",
    "else:\n",
    "    display(Markdown(\"Skipping Docker GPU test\"))\n",
    "    docker_test_successful = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9120e8",
   "metadata": {},
   "source": [
    "## Provide Fix Suggestions\n",
    "\n",
    "Based on the diagnostic results, here are actionable suggestions to fix common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_fixes():\n",
    "    display(Markdown(\"## Diagnostic Summary and Fix Suggestions\"))\n",
    "    \n",
    "    # Check if NVIDIA libraries exist in /usr/lib/wsl/lib\n",
    "    wsl_lib_path = \"/usr/lib/wsl/lib/libnvidia-ml.so\"\n",
    "    wsl_lib_exists = os.path.exists(wsl_lib_path)\n",
    "    \n",
    "    # Check if we found nvidia-smi earlier\n",
    "    nvidia_smi_runs = ran_successfully\n",
    "    \n",
    "    # Check Docker configuration\n",
    "    docker_configured_correctly = docker_configured\n",
    "    \n",
    "    # Print summary\n",
    "    display(Markdown(\"### Status Summary:\"))\n",
    "    display(Markdown(f\"- NVIDIA libraries in WSL path: {'✅ Found' if wsl_lib_exists else '❌ Missing'}\"))\n",
    "    display(Markdown(f\"- nvidia-smi runs successfully: {'✅ Yes' if nvidia_smi_runs else '❌ No'}\"))\n",
    "    display(Markdown(f\"- Docker NVIDIA runtime configured: {'✅ Yes' if docker_configured_correctly else '❌ No'}\"))\n",
    "    \n",
    "    # Generate fix suggestions\n",
    "    display(Markdown(\"### Fix Suggestions:\"))\n",
    "    \n",
    "    fixes = []\n",
    "    \n",
    "    if not wsl_lib_exists and nvidia_smi_runs:\n",
    "        # We have NVIDIA drivers but missing the symbolic link\n",
    "        fixes.append(\"**Missing Library Symbolic Link**:\"\n",
    "                  \"\\n1. Run the fix script: `sudo ./scripts/fix-nvidia-wsl-libs.sh`\"\n",
    "                  \"\\n2. This will create the symbolic link from an existing library to the standard WSL location\")\n",
    "    \n",
    "    if not nvidia_smi_runs:\n",
    "        fixes.append(\"**NVIDIA Driver Issues**:\"\n",
    "                  \"\\n1. Verify you have the NVIDIA driver for WSL installed on Windows (not in WSL)\"\n",
    "                  \"\\n2. Install from: https://developer.nvidia.com/cuda/wsl\"\n",
    "                  \"\\n3. Restart your Windows system after installation\"\n",
    "                  \"\\n4. Make sure your GPU is CUDA-capable and supported by WSL2\")\n",
    "    \n",
    "    if not docker_configured_correctly:\n",
    "        fixes.append(\"**Docker NVIDIA Runtime Configuration**:\"\n",
    "                  \"\\n1. Install NVIDIA Container Toolkit: `sudo apt-get install -y nvidia-container-toolkit`\"\n",
    "                  \"\\n2. Configure Docker: `sudo nvidia-ctk runtime configure --runtime=docker`\"\n",
    "                  \"\\n3. Restart Docker: `sudo systemctl restart docker`\")\n",
    "    \n",
    "    if not fixes:\n",
    "        display(Markdown(\"✅ **Congratulations!** Your NVIDIA GPU appears to be properly configured for WSL and Docker.\"))\n",
    "        display(Markdown(\"You should now be able to use GPU acceleration with Ollama in the CodexContinue project.\"))\n",
    "        display(Markdown(\"Run the following to start with GPU support:\"))\n",
    "        display(Markdown(\"`docker compose -f docker-compose.yml up`\"))\n",
    "    else:\n",
    "        for i, fix in enumerate(fixes):\n",
    "            display(Markdown(f\"#### Fix {i+1}:\\n{fix}\"))\n",
    "    \n",
    "    # Add CodexContinue specific notes\n",
    "    display(Markdown(\"### CodexContinue Project Notes:\"))\n",
    "    display(Markdown(\"1. After fixing GPU issues, test the setup with Ollama:\"))\n",
    "    display(Markdown(\"   ```bash\\n   ./scripts/start-ollama-wsl.sh\\n   ```\"))\n",
    "    display(Markdown(\"2. If problems persist in CodexContinue, use the CPU-only configuration:\"))\n",
    "    display(Markdown(\"   ```bash\\n   docker compose -f docker-compose.yml -f docker-compose.macos.yml up\\n   ```\"))\n",
    "\n",
    "provide_fixes()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
